# 2019-10-16
## Chapter 3 Matrices

_**Def**_ $\text{rank}(A)\equiv\text{rank}(L _A)$, where $A\in M_{m\times n}(F)$ and $L _A:F^n\rightarrow F^m$.

### Theorem whatever

An $n\times n$ matrix $A$ is invertible iff $\text{rank}(A)=n$.

#### Proof

Suppose $A$ is invertible, so is $L _A$.
$\text{rank}(A)=\dim(L _A(F^n))=\dim(F^n)=n$.

Conversely, $\text{rank}(L _A)=n$. Then we know that $\text{nullity}(L _A)=0$, which means that $L _A(F^n)=F^n$.

### Theorem 3.1.1

$T:V\rightarrow W$.
Let $\beta,\gamma$ be ordered basis of $V$ and $W$, respectively.
Then $\text{rank}(T)=\text{rank}\left([T] _\beta^\gamma\right)$.

#### Proof

Consider $\phi _\gamma T=L _A\phi _\beta$, where $A=[T] _\beta^\gamma$.

### Theorem 3.1.2

Let $A$ be a $m\times n$ matrix.
$P$ and $Q$ are invertible $m\times m,n\times n$ matrices.
Then
1. $\text{rank}(AQ)=\text{rank}(A)$
2. $\text{rank}(PA)=\text{rank}(A)$
3. $\text{rank}(PAQ)=\text{rank}(A)$

#### Proof

1. $\dim(L _{AQ}(F^n))=\dim(L _AL _Q(F^n))=\dim(L _A(F^n))$.
2. $\dim(L _{PA}(F^n))=\dim(L _PL _A(F^m))=\dim(L _A(F^n))$.
3. Just apply the previous two principles.

_**Def**_ A $m\times n$ matrix
elementary row operations (**all invertible**)
1. interchange
2. scalar multiplication
3. replacement

Suppose $E$ is a $m\times n$ matrix and $L _E$ is a elementary row operation. Since $EI=E$, we know that $E=L _E(I)$, which helps us calculate $E$. By verifying the result, we know that every elementary row operation can be seen as left-multiplying an invertible matrix. Similarly, every elementary column operations can be seen as right-multiplying an invertible matrix.

Suppose we apply some elementary row operations on a matrix $A$, and receive a reduced echelon form:
$$
E _p\ldots E _2E _1A=A _E,
$$
then we know that $\text{rank}(A)=\text{rank}(A _E)$.

Is # of pivot columns equal to $\text{rank}(A)?$

### Theorem whatever

$\text{rank}(A)=$ maximum # of linearly independent column vectors.

#### Proof

$\text{rank}(A)=\text{rank}(L _A)$.
$A$ is $n\times m$ matrix.
$L _A:F^m\rightarrow F^n$.
$\{e _1,e _2,\ldots,e _m\}$. $R(L _A)=\text{span}\{L _A(e _1),L _A(e _2),\ldots,L _A(e _m)\}$.

Suppose $A=\begin{bmatrix}
a _1&a _2&\cdots&a _m
\end{bmatrix}$. Then $L _A(e _i)=Ae _i=a _i$.
$R(L _A)=\text{span}\{a _1,a _2,\ldots,a _n\}$.

Since all non-pivot columns can be generated by the pivot columns, and the pivot columns are linearly independent, we know that # of pivot columns equals # of linearly independent column vectors. Therefore we know that $\text{rank}(A)=$ # of pivot columns.

In fact, by putting all pivot columns in a correct ordered, we can transform $A$ into the following form:
$$
\begin{pmatrix}
I _{\text{rank}(A)}&B\\
0&0
\end{pmatrix}.
$$
By applying elementary column operations, we can transform $A$ further into the following form:
$$
\begin{pmatrix}
I _{\text{rank}(A)}&0\\
0&0
\end{pmatrix}
$$

Since elementary row operations do not change the maximum # linearly independent row vectors, we know that $\text{rank}(A)=$ maximum # of linearly independent row vectors and that $\text{rank}(A)=\text{rank}(A^t)$.

### Theorem 3.1.5

Let $T:V\rightarrow W$ and $U:W\rightarrow Z$.
Let $A$ and $B$ be matrices such that $AB$ is defined.
1. $\text{rank}(UT)\le\text{rank}(U)$
2. $\text{rank}(UT)\le\text{rank}(T)$
3. $\text{rank}(AB)\le\text{rank}(A)$
4. $\text{rank}(AB)\le\text{rank}(B)$

#### Proof

$1\rightarrow 3\rightarrow 4\rightarrow 2$.

If we have $3$ proved, we know that
$$
\begin{aligned}
\text{rank}(AB)=&\text{rank}\left((AB)^t\right)\\
=&\text{rank}\left(B^tA^t\right)\\
=&\text{rank}\left(B^t\right)=\text{rank}(B).
\end{aligned}
$$

### Partitioned matrices

$A=\begin{bmatrix}
a _1&a _2&\cdots&a _n
\end{bmatrix}$
$A\begin{bmatrix}
x _1\\x _2\\\vdots\\x _n
\end{bmatrix}=a _1x _1+a _2x _2+\cdots+a _nx _n$
$AB=A\begin{bmatrix}
b _1&b _2&\cdots&b _n
\end{bmatrix}=\begin{bmatrix}
Ab _1&Ab _2&\cdots&Ab _b
\end{bmatrix}$

### Examples

Prove $A^2=A$ ($A~n\times n$) iff $\text{rank}(A)+\text{rank}(I _n-A)=n$.

#### Proof

$\text{rank}(A)+\text{rank}(I _n-A)=\text{rank}\begin{pmatrix}
A&0\\
0&I _n-A
\end{pmatrix}=\text{rank}\begin{pmatrix}
A^2-A&0\\
0&I _n
\end{pmatrix}$

$$
\begin{aligned}
&\begin{pmatrix}
A&0\\
0&I _n-A
\end{pmatrix}\\
\sim&\begin{pmatrix}
A&A\\
0&I _n-A
\end{pmatrix}\\
\sim&\begin{pmatrix}
A&A\\
A&I _n
\end{pmatrix}\\
\sim&\begin{pmatrix}
A-A^2&A\\
0&I _n
\end{pmatrix}\\
\sim&\begin{pmatrix}
A-A^2&0\\
0&I _n
\end{pmatrix}\\
\sim&\begin{pmatrix}
A^2-A&0\\
0&I _n
\end{pmatrix}
\end{aligned}
$$

### LU decomposition

$A$ is a $m\times n$ matrix,
$Ax=b$,
if we can decompose $A$ as $LU$ that is
$A=\begin{pmatrix}
1&0&\cdots&0\\
*&1&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
*& *&\cdots&1
\end{pmatrix} _{m\times m}U$, where U is in echelon form,
then we can divide the linear equation system into
$$
\left\{\begin{matrix}
Ly&=&b\\
Ux&=&y
\end{matrix}\right.
$$

First apply elementary row operations on $A$ such that $EA=U$, then we have $A=E^{-1}U$. If we didn't apply exchange operations, we can prove that $L=E^{-1}$ is in the required form. We know that $EL=I _m$, that is, applying the same operations on $L$ will result in $I _m$. Let $Q _i$ be the statement saying that $\forall j\gt i,L _{ij}=0$. We can first prove $Q _1$, then $Q _2$, ...

### LUP decomposition (?)

If we need to apply exchange operations, suppose they are $P _1,P _2,\ldots,P _s$, then $P _sP _{s-1}\cdots P _1A$ can be transformed into an echelon form without exchange operations. Thus we have $PA=LU$.

---

# 2019-10-23

Consider $Ax=b$, if $b=0$, then we call it homogeneous, otherwise we call it nonhomogeneous.

### Theorem 3.3.1

Let $K$ be the  solution set of $Ax=b$, and let $K _H$ be the solution of $Ax=0$. Then
$$
K'=\{s\}+K _H=\{s+k: k\in K _n\},
$$
where $s$ is any solution to $Ax=b$.
$K'=K$.

#### Proof

$\forall y\in K'$,
$y=s+k,k\in K _H$,
$Ay=A(s+k)=b$,
$y\in K$,
$K'\subseteq K$.

$\forall w\in K$,
$Aw=b,As=b\Rightarrow A(w-s)=0$,
$w-s\in K _H$,
$\exist h\in K _H,w-s=h$,
$w=s+h\in K'$,
$K\subseteq K'$.

### Theorem 3.3.2

Let $A$ be an $n\times n $ matrix. $Ax=b$. If $A$ is invertible, then the system has exactly one solution $x=A^{-1}b$. Conversely, if the system has only one solution, then $A$ is invertible.

#### Proof

Suppose $A$ is invertible. Trivial.

Suppose the system has only one solution, then $K _H=\{0\}$.
$Ax=0\Rightarrow \text{rank}(A)=0$.

### Theorem 3.3.3

$Ax=b$.
The system is consistent iff
$\text{rank}(A\mid b)=\text{rank}(A)$.

#### Proof

$A=\begin{bmatrix}
a _1&a _2&\cdots&a _n
\end{bmatrix},$
$Ax=a _1x _1+\cdots+a _nx _n=b$.

Suppose the system is consistent, then $b$ is a linear combination of $a _1,a _2,\ldots,a _n$, which means that $\text{rank}(A\mid b)=\text{rank}(A)$.

Conversely, $\text{rank}(A\mid b)=\text{rank}(A)$, that is, adding $b$ to $a _1,a _2,\ldots,a _n$ doesn't change the maximum # of linearly independent vectors. Therefore we know that $\exist \{x _n\},b=a _1x _1+a _2x _2+\cdots+a _nx _n$.

## Determinant

_**Def**_ $M _{n\times n}(F)\rightarrow F$
$\det\begin{pmatrix}
a&b\\c&d
\end{pmatrix}=ad-bc$

If we see it as column vectors $v _1=(a,c)$ and $v _2=(b,d)$, then $\det\begin{pmatrix}
a&b\\c&d
\end{pmatrix}=v _1\times v _2$.

Let $A\in M _{2\times 2}(F)$. Then
$\det(A)$ is nonzero iff $A$ is invertible.

#### Proof

If $\det(A)\neq 0,A=\begin{bmatrix}
v _1&v _2
\end{bmatrix}$,
$\{v _1,v _2\}$ is linearly independent since $S=v _1\times v _2=\text{rank}(A)\neq 0$.

Conversely, if $A$ is invertible, then $S=v _1\times v _2\neq 0\Rightarrow\det(A)\neq 0$.

---

Let $\tilde A _{ij}$ be the result of deleting the $i$th row and $j$th column of $A$.

$$\det(A)\equiv\sum _{j=1}^n(-1)^{1+j}\det\left(\tilde A _{1j}\right)A _{ij}.$$

$(-1)^{1+j}\det\left(\tilde A _{ij}\right)$ is called the cofactor of entry $A _{ij}$, and the equation is called the cofactor expansion along the first row.

### Theorem 4.1.1

$a _i,u,v\in F^n$.
$\det\begin{pmatrix}
a _1\\\vdots\\a _{r-1}\\u+\lambda v\\a _{r+1}\\\vdots\\a _n
\end{pmatrix}=\det\begin{pmatrix}
a _1\\\vdots\\a _{r-1}\\u\\a _{r+1}\\\vdots\\a _n
\end{pmatrix}+\lambda\det\begin{pmatrix}
a _1\\\vdots\\a _{r-1}\\v\\a _{r+1}\\\vdots\\a _n
\end{pmatrix}.$

#### Proof

$r=1$, trivial.

$r\neq 1$.
If $n=2$, the statement holds.
Suppose $n\gt 2$, the statement is true,
let's prove the case $n+1$.
$$
\begin{aligned}
\det\begin{pmatrix}
a _1\\\vdots\\a _{r-1}\\u+\lambda v\\a _{r+1}\\\vdots\\a _{n+1}
\end{pmatrix}=&\sum _{j=1}^{n+1}(-1)^{1+j}a _{ij}\det\left(\tilde A _{ij}(u+\lambda v)\right)\\
=&\sum _{j}(-1)^{1+j}\left((-1)^{1+j}\det\left(\tilde A _{1j}\right)(u)+(-1)^{1+j}\lambda\det\left(\tilde A _{1j}(v)\right)\right)
\end{aligned}
$$

# 2019-10-28
### Lemma

Let $B\in M _{n\times n}(F)~(n\ge 2)$. If row $i$ of $B$ equals $e _k$ for some $k~(1\le k\le n)$. Then $\det(B)=(-1)^{i+k}\det{\tilde B _{i,k}}$. (Therefore, we can perform the cofactor expansion along any row.)

#### Proof

If $n=2$, trivial.

Suppose the equation holds for a $n\times n$ matrix. Let's prove the case for $(n+1)\times(n+1)$ matrix A. Row $i$ is $e _k$. Let $C=\tilde A _{i,k}$.
$$
\begin{aligned}
\det(A)=&\sum _{j\lt k}(-1)^{1+j}A _{1j}\det(\tilde A _{1j})\\
&+(-1)^{1+k}A _{1k}\det(\tilde A _{1k})+\sum _{j\gt k}(-1)^{1+j}A _{1j}\det(\tilde A _{1j})\\
=&\sum _{j=1}^{k-1}(-1)^{1+j}(-1)^{i+k}C _{1j}\det(\tilde C _{1j})\\
&+\sum _{j=k+1}^{n+1}(-1)^{1+j}C _{1,j-1}(-1)^{i+k-1}\det(\tilde C _{1,j-1})\\
=&(-1)^{i+k}\det(C).
\end{aligned}
$$
<!--  -->